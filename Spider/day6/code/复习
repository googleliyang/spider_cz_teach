反馈
  1.能给我们讲讲开发大型爬虫项目得主要流程么?
    n目标站点(百度贴吧/微博/天涯/)
        1.各自实现自己划分的目标站点的爬虫
        2.实现爬虫管理
            1.爬虫基础知识实现(没有使用框架)
                flask,django,php实现一个爬虫管理站点
            2.框架
                scrapyd项目部署管理
                    gerapy
        3.与其他平台的交互
            java hadoop平台
                分布式数据存储 hdfs        视频存储
                数据存储于hbase
                kafka
                zookeeper
                hive
            机器学习/数据挖掘
        4.完善爬虫的稳定性
            网络      自稳
            日志      logging
            断点续爬      100w url 80w url
            去重
                地址去重
                    url   容量有限
                    url - hash
                    布隆过滤器  20
                内容去重
                    编辑距离算法
                    simhash

  2.老师，如果python 的web没有找到工作，转测试好不好转 测试前景咋样 说说

复习
    1.scrapy开发流程
        1.创建项目
            scrapy startproject project_name
        2.明确目标
            items.py文件建模  scrpay.Field()

        3.创建爬虫
            1.创建爬虫文件
                scrapy genspider [-t crawl] name domains
            2.编写爬虫
                1.检查修改起始的url
                2.检查修改允许的域
                3.编写解析方法
        4.保存内容
            1.在pipeline文件中定义管道类
                process_item
            2.在settings文件中注册使用管道
    2.请求
        get请求  Request类
            url,callback,method="GET",headers,body,cookies,meta={},dont_filter=False
        post请求 FormRequest类
            url,callback,formdata,headers,cookies,meta,dont_filter=False

    3.中间件的使用
        在middlewares.py文件中编写中间件
            process_request(self, request, spider)
            process_response(self, response, request, spider)
        在settings文件中注册使用中间件